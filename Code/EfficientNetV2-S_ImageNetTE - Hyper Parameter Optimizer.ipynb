{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09950f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from clearml import Task\n",
    "from clearml.automation import (\n",
    "    DiscreteParameterRange, HyperParameterOptimizer, RandomSearch,\n",
    "    UniformIntegerParameterRange, UniformParameterRange)\n",
    "\n",
    "%env CLEARML_WEB_HOST=https://app.clear.ml\n",
    "%env CLEARML_API_HOST=https://api.clear.ml\n",
    "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
    "# Pytorch\n",
    "%env CLEARML_API_ACCESS_KEY=API_KEY\n",
    "%env CLEARML_API_SECRET_KEY=API_SECRET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f6dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to load Bayesian optimizer package\n",
    "try:\n",
    "    from clearml.automation.optuna import OptimizerOptuna  # noqa\n",
    "    aSearchStrategy = OptimizerOptuna\n",
    "except ImportError as ex:\n",
    "    try:\n",
    "        from clearml.automation.hpbandster import OptimizerBOHB  # noqa\n",
    "        aSearchStrategy = OptimizerBOHB\n",
    "    except ImportError as ex:\n",
    "        logging.getLogger().warning(\n",
    "            'Apologies, it seems you do not have \\'optuna\\' or \\'hpbandster\\' installed, '\n",
    "            'we will be using RandomSearch strategy instead')\n",
    "        aSearchStrategy = RandomSearch\n",
    "\n",
    "\n",
    "def job_complete_callback(\n",
    "    job_id,                 # type: str\n",
    "    objective_value,        # type: float\n",
    "    objective_iteration,    # type: int\n",
    "    job_parameters,         # type: dict\n",
    "    top_performance_job_id  # type: str\n",
    "):\n",
    "    print('Job completed!', job_id, objective_value, objective_iteration, job_parameters)\n",
    "    if job_id == top_performance_job_id:\n",
    "        print('WOOT WOOT we broke the record! Objective reached {}'.format(objective_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4647db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting ClearML with the current process,\n",
    "# from here on everything is logged automatically\n",
    "task = Task.init(project_name='ImageNetTE',\n",
    "                 task_name='ImageNetTE - EfficientNetV2-S - Image Sizes',\n",
    "                 task_type=Task.TaskTypes.optimizer,\n",
    "                 reuse_last_task_id=False)\n",
    "\n",
    "# Change this to change the task on which you base the hyper parameter tuning\n",
    "template_id = \"78bb69f6f721430ab06c76fcd7741830\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79ccd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the hyper parameters which are randomly optimized\n",
    "hyper_parameters = [\n",
    "    DiscreteParameterRange('Stage Params/dropout_min', values=[0.1, 0.2]),\n",
    "    DiscreteParameterRange('Stage Params/dropout_max', values=[ 0.3, 0.5, 0.7]),\n",
    "    #DiscreteParameterRange('Stage Params/image_size_min', values=[128, 200]),\n",
    "    #DiscreteParameterRange('Stage Params/image_size_max', values=[200, 300, 400]),\n",
    "    DiscreteParameterRange('Stage Params/epochs', values=[20]),\n",
    "    #DiscreteParameterRange('Stage Params/epochs', values=[20, 40, 60]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f270c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default queue name for the Training tasks themselves.\n",
    "# later can be overridden in the UI\n",
    "execution_queue = 'default'\n",
    "\n",
    "# Example use case:\n",
    "an_optimizer = HyperParameterOptimizer(\n",
    "    # This is the experiment we want to optimize\n",
    "    base_task_id=template_id,\n",
    "    # here we define the hyper-parameters to optimize\n",
    "    # Notice: The parameter name should exactly match what you see in the UI: <section_name>/<parameter>\n",
    "    # For Example, here we see in the base experiment a section Named: \"General\"\n",
    "    # under it a parameter named \"batch_size\", this becomes \"General/batch_size\"\n",
    "    # If you have `argparse` for example, then arguments will appear under the \"Args\" section,\n",
    "    # and you should instead pass \"Args/batch_size\"\n",
    "    hyper_parameters=hyper_parameters,\n",
    "    # this is the objective metric we want to maximize/minimize\n",
    "    objective_metric_title='Accuracy',\n",
    "    objective_metric_series='Stage 4: validation',\n",
    "    # now we decide if we want to maximize it or minimize it (accuracy we maximize)\n",
    "    objective_metric_sign='max',\n",
    "    # let us limit the number of concurrent experiments,\n",
    "    # this in turn will make sure we do dont bombard the scheduler with experiments.\n",
    "    # if we have an auto-scaler connected, this, by proxy, will limit the number of machine\n",
    "    max_number_of_concurrent_tasks=1,\n",
    "    # this is the optimizer class (actually doing the optimization)\n",
    "    # Currently, we can choose from GridSearch, RandomSearch or OptimizerBOHB (Bayesian optimization Hyper-Band)\n",
    "    # more are coming soon...\n",
    "    optimizer_class=aSearchStrategy,\n",
    "    # Select an execution queue to schedule the experiments for execution\n",
    "    execution_queue=execution_queue,\n",
    "    # If specified all Tasks created by the HPO process will be created under the `spawned_project` project\n",
    "    spawn_project=None,\n",
    "    # If specified only the top K performing Tasks will be kept, the others will be automatically archived\n",
    "    save_top_k_tasks_only=3,\n",
    "    # Optional: Limit the execution time of a single experiment, in minutes.\n",
    "    # (this is optional, and if using  OptimizerBOHB, it is ignored)\n",
    "    time_limit_per_job=200.,\n",
    "    # Check the experiments every 60 seconds\n",
    "    pool_period_min=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ecedbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report every 60 seconds\n",
    "an_optimizer.set_report_period(1)\n",
    "# start the optimization process, callback function to be called every time an experiment is completed\n",
    "# this function returns immediately\n",
    "an_optimizer.start(job_complete_callback=job_complete_callback)\n",
    "# You can also use the line below instead to run all the optimizer tasks locally, without using queues or agent\n",
    "# an_optimizer.start_locally(job_complete_callback=job_complete_callback)\n",
    "# set the time limit for the optimization process (2 hours)\n",
    "an_optimizer.set_time_limit(in_minutes=1200.0)\n",
    "# wait until process is done (notice we are controlling the optimization process in the background)\n",
    "an_optimizer.wait()\n",
    "# optimization is completed, print the top performing experiments id\n",
    "top_exp = an_optimizer.get_top_experiments(top_k=3)\n",
    "print([t.id for t in top_exp])\n",
    "# make sure background optimization stopped\n",
    "an_optimizer.stop()\n",
    "\n",
    "print('We are done, good bye')\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d4b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
